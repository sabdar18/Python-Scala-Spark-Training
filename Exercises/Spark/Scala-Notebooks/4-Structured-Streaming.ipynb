{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bae5ef2081fd:4041\n",
       "SparkContext available as 'sc' (version = 3.0.0-preview2, master = local[*], app id = local-1620524395237)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "static: org.apache.spark.sql.DataFrame = [Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]\n",
       "dataSchema: org.apache.spark.sql.types.StructType = StructType(StructField(Arrival_Time,LongType,true), StructField(Creation_Time,LongType,true), StructField(Device,StringType,true), StructField(Index,LongType,true), StructField(Model,StringType,true), StructField(User,StringType,true), StructField(gt,StringType,true), StructField(x,DoubleType,true), StructField(y,DoubleType,true), StructField(z,DoubleType,true))\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val static = spark.read.json(\"/spark-data/activity-data/\")\n",
    "val dataSchema = static.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "streaming: org.apache.spark.sql.DataFrame = [Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val streaming = spark.readStream.schema(dataSchema)\n",
    "  .option(\"maxFilesPerTrigger\", 1).json(\"/spark-data/activity-data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activityCounts: org.apache.spark.sql.DataFrame = [gt: string, count: bigint]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val activityCounts = streaming.groupBy(\"gt\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activityQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7205c6a0\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\n",
    "  .format(\"memory\").outputMode(\"complete\")\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "//activityQuery.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[org.apache.spark.sql.streaming.StreamingQuery] = Array(org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7205c6a0)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|86162|\n",
      "|     stand|79694|\n",
      "|stairsdown|65554|\n",
      "|      walk|92792|\n",
      "|  stairsup|73165|\n",
      "|      null|73136|\n",
      "|      bike|75580|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|123085|\n",
      "|     stand|113850|\n",
      "|stairsdown| 93648|\n",
      "|      walk|132560|\n",
      "|  stairsup|104523|\n",
      "|      null|104480|\n",
      "|      bike|107973|\n",
      "+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|160006|\n",
      "|     stand|148005|\n",
      "|stairsdown|121733|\n",
      "|      walk|172328|\n",
      "|  stairsup|135887|\n",
      "|      null|135826|\n",
      "|      bike|140370|\n",
      "+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|209235|\n",
      "|     stand|193550|\n",
      "|stairsdown|159178|\n",
      "|      walk|225351|\n",
      "|  stairsup|177714|\n",
      "|      null|177615|\n",
      "|      bike|183559|\n",
      "+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|246159|\n",
      "|     stand|227709|\n",
      "|stairsdown|187264|\n",
      "|      walk|265119|\n",
      "|  stairsup|209082|\n",
      "|      null|208954|\n",
      "|      bike|215949|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "for( i <- 1 to 5 ) {\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    Thread.sleep(1000)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.expr\n",
       "simpleTransform: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@77cb72da\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.expr\n",
    "val simpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\n",
    "  .where(\"stairs\")\n",
    "  .where(\"gt is not null\")\n",
    "  .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\n",
    "  .writeStream\n",
    "  .queryName(\"simple_transform\")\n",
    "  .format(\"memory\")\n",
    "  .outputMode(\"append\")\n",
    "  .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deviceModelStats: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@75907124\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\n",
    "  .drop(\"avg(Arrival_time)\")\n",
    "  .drop(\"avg(Creation_Time)\")\n",
    "  .drop(\"avg(Index)\")\n",
    "  .writeStream.queryName(\"device_counts\").format(\"memory\").outputMode(\"complete\")\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.IllegalArgumentException",
     "evalue": " Cannot start query with name device_counts as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "java.lang.IllegalArgumentException: Cannot start query with name device_counts as a query with that name is already active in this SparkSession",
      "  at org.apache.spark.sql.streaming.StreamingQueryManager.$anonfun$startQuery$1(StreamingQueryManager.scala:356)",
      "  at org.apache.spark.sql.streaming.StreamingQueryManager.$anonfun$startQuery$1$adapted(StreamingQueryManager.scala:353)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:353)",
      "  at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:269)",
      "  ... 37 elided",
      ""
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "val historicalAgg = static.groupBy(\"gt\", \"model\").avg()\n",
    "val deviceModelStats = streaming.drop(\"Arrival_Time\", \"Creation_Time\", \"Index\")\n",
    "  .cube(\"gt\", \"model\").avg()\n",
    "  .join(historicalAgg, Seq(\"gt\", \"model\"))\n",
    "  .writeStream.queryName(\"device_counts\").format(\"memory\").outputMode(\"complete\")\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "// Subscribe to 1 topic\n",
    "val ds1 = spark.readStream.format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "  .option(\"subscribe\", \"topic1\")\n",
    "  .load()\n",
    "// Subscribe to multiple topics\n",
    "val ds2 = spark.readStream.format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "  .option(\"subscribe\", \"topic1,topic2\")\n",
    "  .load()\n",
    "// Subscribe to a pattern of topics\n",
    "val ds3 = spark.readStream.format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "  .option(\"subscribePattern\", \"topic.*\")\n",
    "  .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "ds1.selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "  .writeStream.format(\"kafka\")\n",
    "  .option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "  .start()\n",
    "ds1.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "  .writeStream.format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "  .option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\n",
    "  .option(\"topic\", \"topic1\")\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//in Scala\n",
    "datasetOfString.write.foreach(new ForeachWriter[String] {\n",
    "  def open(partitionId: Long, version: Long): Boolean = {\n",
    "    // open a database connection\n",
    "  }\n",
    "  def process(record: String) = {\n",
    "    // write string to connection\n",
    "  }\n",
    "  def close(errorOrNull: Throwable): Unit = {\n",
    "    // close the connection\n",
    "  }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "val socketDF = spark.readStream.format(\"socket\")\n",
    "  .option(\"host\", \"localhost\").option(\"port\", 9999).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts.format(\"console\").write()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "activityCounts.writeStream.format(\"memory\").queryName(\"my_device_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "activityCounts.writeStream.trigger(Trigger.ProcessingTime(\"100 seconds\"))\n",
    "  .format(\"console\").outputMode(\"complete\").start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "activityCounts.writeStream.trigger(Trigger.Once())\n",
    "  .format(\"console\").outputMode(\"complete\").start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String,\n",
    "  count: BigInt)\n",
    "val dataSchema = spark.read\n",
    "  .parquet(\"/data/flight-data/parquet/2010-summary.parquet/\")\n",
    "  .schema\n",
    "val flightsDF = spark.readStream.schema(dataSchema)\n",
    "  .parquet(\"/data/flight-data/parquet/2010-summary.parquet/\")\n",
    "val flights = flightsDF.as[Flight]\n",
    "def originIsDestination(flight_row: Flight): Boolean = {\n",
    "  return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME\n",
    "}\n",
    "flights.filter(flight_row => originIsDestination(flight_row))\n",
    "  .groupByKey(x => x.DEST_COUNTRY_NAME).count()\n",
    "  .writeStream.queryName(\"device_counts\").format(\"memory\").outputMode(\"complete\")\n",
    "  .start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
