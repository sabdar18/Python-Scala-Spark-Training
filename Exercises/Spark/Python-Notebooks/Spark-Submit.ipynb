{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark\r\n"
     ]
    }
   ],
   "source": [
    "! echo $SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/05/11 00:12:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/05/11 00:12:33 INFO SparkContext: Running Spark version 3.0.0-preview2\n",
      "21/05/11 00:12:33 INFO ResourceUtils: ==============================================================\n",
      "21/05/11 00:12:33 INFO ResourceUtils: Resources for spark.driver:\n",
      "\n",
      "21/05/11 00:12:33 INFO ResourceUtils: ==============================================================\n",
      "21/05/11 00:12:33 INFO SparkContext: Submitted application: Word Count\n",
      "21/05/11 00:12:33 INFO SecurityManager: Changing view acls to: u1\n",
      "21/05/11 00:12:33 INFO SecurityManager: Changing modify acls to: u1\n",
      "21/05/11 00:12:33 INFO SecurityManager: Changing view acls groups to: \n",
      "21/05/11 00:12:33 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/05/11 00:12:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(u1); groups with view permissions: Set(); users  with modify permissions: Set(u1); groups with modify permissions: Set()\n",
      "21/05/11 00:12:33 INFO Utils: Successfully started service 'sparkDriver' on port 46135.\n",
      "21/05/11 00:12:33 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/05/11 00:12:33 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/05/11 00:12:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/05/11 00:12:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/05/11 00:12:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/05/11 00:12:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e7f3ba77-3c73-419b-98ac-840310670ac4\n",
      "21/05/11 00:12:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "21/05/11 00:12:34 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/05/11 00:12:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/05/11 00:12:34 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "21/05/11 00:12:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://bae5ef2081fd:4041\n",
      "21/05/11 00:12:34 INFO Executor: Starting executor ID driver on host bae5ef2081fd\n",
      "21/05/11 00:12:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36005.\n",
      "21/05/11 00:12:34 INFO NettyBlockTransferService: Server created on bae5ef2081fd:36005\n",
      "21/05/11 00:12:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/05/11 00:12:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, bae5ef2081fd, 36005, None)\n",
      "21/05/11 00:12:34 INFO BlockManagerMasterEndpoint: Registering block manager bae5ef2081fd:36005 with 366.3 MiB RAM, BlockManagerId(driver, bae5ef2081fd, 36005, None)\n",
      "21/05/11 00:12:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, bae5ef2081fd, 36005, None)\n",
      "21/05/11 00:12:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, bae5ef2081fd, 36005, None)\n",
      "/opt/spark/python/lib/pyspark.zip/pyspark/context.py:219: DeprecationWarning: Support for Python 2 and Python 3 prior to version 3.6 is deprecated as of Spark 3.0. See also the plan for dropping Python 2 support at https://spark.apache.org/news/plan-for-dropping-python-2-support.html.\n",
      "  DeprecationWarning)\n",
      "21/05/11 00:12:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/u1/Python-Scala-Spark-Training/Exercises/Spark/Python-Notebooks/spark-warehouse/').\n",
      "21/05/11 00:12:35 INFO SharedState: Warehouse path is 'file:/home/u1/Python-Scala-Spark-Training/Exercises/Spark/Python-Notebooks/spark-warehouse/'.\n",
      "21/05/11 00:12:38 INFO CodeGenerator: Code generated in 213.807182 ms\n",
      "21/05/11 00:12:38 INFO SparkContext: Starting job: collect at /home/u1/Python-Scala-Spark-Training/Exercises/Spark/Python-Notebooks/main.py:9\n",
      "21/05/11 00:12:39 INFO DAGScheduler: Got job 0 (collect at /home/u1/Python-Scala-Spark-Training/Exercises/Spark/Python-Notebooks/main.py:9) with 1 output partitions\n",
      "21/05/11 00:12:39 INFO DAGScheduler: Final stage: ResultStage 0 (collect at /home/u1/Python-Scala-Spark-Training/Exercises/Spark/Python-Notebooks/main.py:9)\n",
      "21/05/11 00:12:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/05/11 00:12:39 INFO DAGScheduler: Missing parents: List()\n",
      "21/05/11 00:12:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at collect at /home/u1/Python-Scala-Spark-Training/Exercises/Spark/Python-Notebooks/main.py:9), which has no missing parents\n",
      "21/05/11 00:12:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 13.8 KiB, free 366.3 MiB)\n",
      "21/05/11 00:12:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 366.3 MiB)\n",
      "21/05/11 00:12:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on bae5ef2081fd:36005 (size: 5.7 KiB, free: 366.3 MiB)\n",
      "21/05/11 00:12:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1206\n",
      "21/05/11 00:12:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at collect at /home/u1/Python-Scala-Spark-Training/Exercises/Spark/Python-Notebooks/main.py:9) (first 15 tasks are for partitions Vector(0))\n",
      "21/05/11 00:12:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "21/05/11 00:12:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, bae5ef2081fd, executor driver, partition 0, PROCESS_LOCAL, 7393 bytes)\n",
      "21/05/11 00:12:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "21/05/11 00:12:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1958 bytes result sent to driver\n",
      "21/05/11 00:12:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 354 ms on bae5ef2081fd (executor driver) (1/1)\n",
      "21/05/11 00:12:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21/05/11 00:12:39 INFO DAGScheduler: ResultStage 0 (collect at /home/u1/Python-Scala-Spark-Training/Exercises/Spark/Python-Notebooks/main.py:9) finished in 0.510 s\n",
      "21/05/11 00:12:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/05/11 00:12:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "21/05/11 00:12:39 INFO DAGScheduler: Job 0 finished: collect at /home/u1/Python-Scala-Spark-Training/Exercises/Spark/Python-Notebooks/main.py:9, took 0.555040 s\n",
      "[Row(sum(id)=12372250)]\n",
      "21/05/11 00:12:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on bae5ef2081fd:36005 in memory (size: 5.7 KiB, free: 366.3 MiB)\n",
      "21/05/11 00:12:39 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "21/05/11 00:12:39 INFO SparkUI: Stopped Spark web UI at http://bae5ef2081fd:4041\n",
      "21/05/11 00:12:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/05/11 00:12:39 INFO MemoryStore: MemoryStore cleared\n",
      "21/05/11 00:12:39 INFO BlockManager: BlockManager stopped\n",
      "21/05/11 00:12:39 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/05/11 00:12:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/05/11 00:12:39 INFO SparkContext: Successfully stopped SparkContext\n",
      "21/05/11 00:12:39 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/05/11 00:12:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-e5f8f4cb-fed5-498c-b593-5a00f5d212e9\n",
      "21/05/11 00:12:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-4b530840-4b7b-42aa-affc-b6d2df3a7362\n",
      "21/05/11 00:12:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-e5f8f4cb-fed5-498c-b593-5a00f5d212e9/pyspark-81c79932-78c6-4eb2-83fd-b63d61a4f6bd\n"
     ]
    }
   ],
   "source": [
    "! $SPARK_HOME/bin/spark-submit --master local main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
